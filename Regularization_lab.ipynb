{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from the last lab that we had a training accuracy close to 90% and a test set accuracy close to 76%.\n",
    "\n",
    "2 questions:\n",
    "- Is there a high bias? yes/no\n",
    "- Is there a high variance? yes/no\n",
    "\n",
    "Recall that \"high bias\" is a relative concept. Knowing we have 7 classes and the topics are related, we'll assume that a 90% accuracy is pretty good and the bias on the training set is low. In this lab, we'll use the notion of training/validation/test set to get better insights of how we can mitigate our variance, and we'll look at a few regularization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything in section 2 of this lab is basically what you have done in the previous lab with respect to preprocessing the data. We'll do that again here, and then we'll rerun our model but this time:\n",
    "- We'll include a validation set as well, and start with using early stopping to minimize the discrepancy between train and test accuracy.\n",
    "- We'll use L1 and L2 regularization.\n",
    "- We'll use dropout regularization.\n",
    "- We'll look at the effect of using more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pre-processing the bank complaints as in the previous lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Import the libraries and take a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>In XX/XX/XXXX I filled out the Fedlaon applica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am being contacted by a debt collector for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I cosigned XXXX student loans at SallieMae for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>Navient has sytematically and illegally failed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>My wife became eligible for XXXX Loan Forgiven...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Product                       Consumer complaint narrative\n",
       "0  Student loan  In XX/XX/XXXX I filled out the Fedlaon applica...\n",
       "1  Student loan  I am being contacted by a debt collector for p...\n",
       "2  Student loan  I cosigned XXXX student loans at SallieMae for...\n",
       "3  Student loan  Navient has sytematically and illegally failed...\n",
       "4  Student loan  My wife became eligible for XXXX Loan Forgiven..."
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(123)\n",
    "df = df.sample(10000)\n",
    "df.index = range(10000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 7 types of complaints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 2 columns):\n",
      "Product                         10000 non-null object\n",
      "Consumer complaint narrative    10000 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 156.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 One-hot encoding of the complaints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only keep 2,000 most common words and use one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(complaints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2000)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 One-hot encoding of the products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use LabelEncoder to encode the labels (From category names to category numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bank account or service',\n",
       " 'Checking or savings account',\n",
       " 'Consumer Loan',\n",
       " 'Credit card',\n",
       " 'Credit reporting',\n",
       " 'Mortgage',\n",
       " 'Student loan']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " list(le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform `product` into a numeric vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 1, 2, ..., 5, 2, 6])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_cat = le.transform(product) \n",
    "product_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, use onehot encoding to get to vectors with 0's and 1's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_onehot = to_categorical(product_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 7)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(product_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Train - test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(123)\n",
    "test_index = random.sample(range(1,10000), 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 7)\n",
      "(8500, 7)\n",
      "(1500, 2000)\n",
      "(8500, 2000)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(label_test))\n",
    "print(np.shape(label_train))\n",
    "print(np.shape(test))\n",
    "print(np.shape(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running the model using a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Creating the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we mentioned that in deep learning, we generally keep aside a validation set, which is used during hyperparameter tuning. Then when we have made the final model decision, the test set is used to define the final model perforance. \n",
    "\n",
    "In this example, let's take the first 1000 cases out of the training set to become the validation set. You should do this for both `train` and `label_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "val = train[:1000]\n",
    "train_final = train[1000:]\n",
    "label_val = label_train[:1000]\n",
    "label_train_final = label_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Creating, compiling and running the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rebuild a fully connected (Dense) layer network with relu activations in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because we are dealing with a multiclass problem (classifying the complaints into 7 classes), we use a use a softmax classifyer in order to output 7 class probabilities per case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same settings as before to compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the compiler, you'll be passing the optimizer, loss function, and metrics. Train the model for 120 epochs in mini-batches of 256 samples. This time, let's include the argument `validation_data` and assign it `(val, label_val)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 1.9229 - acc: 0.2043 - val_loss: 1.9183 - val_acc: 0.1990\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.8940 - acc: 0.2269 - val_loss: 1.8931 - val_acc: 0.2130\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.8668 - acc: 0.2431 - val_loss: 1.8655 - val_acc: 0.2270\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.8360 - acc: 0.2629 - val_loss: 1.8319 - val_acc: 0.2380\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.7991 - acc: 0.2900 - val_loss: 1.7915 - val_acc: 0.2680\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.7558 - acc: 0.3209 - val_loss: 1.7430 - val_acc: 0.3190\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.7065 - acc: 0.3632 - val_loss: 1.6905 - val_acc: 0.3510\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.6525 - acc: 0.4101 - val_loss: 1.6337 - val_acc: 0.4000\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.5940 - acc: 0.4525 - val_loss: 1.5738 - val_acc: 0.4460\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.5331 - acc: 0.5024 - val_loss: 1.5115 - val_acc: 0.4910\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.4705 - acc: 0.5415 - val_loss: 1.4497 - val_acc: 0.5190\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.4082 - acc: 0.5692 - val_loss: 1.3896 - val_acc: 0.5510\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.3491 - acc: 0.5980 - val_loss: 1.3324 - val_acc: 0.5830\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.2924 - acc: 0.6184 - val_loss: 1.2791 - val_acc: 0.6070\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.2390 - acc: 0.6344 - val_loss: 1.2286 - val_acc: 0.6260\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.1884 - acc: 0.6532 - val_loss: 1.1835 - val_acc: 0.6190\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.1417 - acc: 0.6705 - val_loss: 1.1396 - val_acc: 0.6390\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.0981 - acc: 0.6789 - val_loss: 1.1007 - val_acc: 0.6380\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.0581 - acc: 0.6901 - val_loss: 1.0628 - val_acc: 0.6590\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.0209 - acc: 0.6996 - val_loss: 1.0307 - val_acc: 0.6620\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9868 - acc: 0.7067 - val_loss: 0.9991 - val_acc: 0.6750\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9549 - acc: 0.7184 - val_loss: 0.9712 - val_acc: 0.6780\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.9256 - acc: 0.7203 - val_loss: 0.9472 - val_acc: 0.6840\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8985 - acc: 0.7295 - val_loss: 0.9200 - val_acc: 0.6900\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8736 - acc: 0.7327 - val_loss: 0.8991 - val_acc: 0.6960\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8501 - acc: 0.7360 - val_loss: 0.8791 - val_acc: 0.6970\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8282 - acc: 0.7413 - val_loss: 0.8629 - val_acc: 0.6950\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8085 - acc: 0.7468 - val_loss: 0.8459 - val_acc: 0.7040\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.7901 - acc: 0.7504 - val_loss: 0.8307 - val_acc: 0.7060\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.7729 - acc: 0.7535 - val_loss: 0.8159 - val_acc: 0.7130\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.7570 - acc: 0.7600 - val_loss: 0.8036 - val_acc: 0.7140\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.7417 - acc: 0.7612 - val_loss: 0.7915 - val_acc: 0.7080\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.7282 - acc: 0.7648 - val_loss: 0.7822 - val_acc: 0.7080\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.7150 - acc: 0.7664 - val_loss: 0.7707 - val_acc: 0.7200\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.7023 - acc: 0.7713 - val_loss: 0.7631 - val_acc: 0.7090\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.6909 - acc: 0.7729 - val_loss: 0.7520 - val_acc: 0.7190\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.6802 - acc: 0.7749 - val_loss: 0.7443 - val_acc: 0.7160\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.6692 - acc: 0.7779 - val_loss: 0.7363 - val_acc: 0.7240\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.6596 - acc: 0.7812 - val_loss: 0.7328 - val_acc: 0.7190\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.6501 - acc: 0.7852 - val_loss: 0.7251 - val_acc: 0.7250\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.6415 - acc: 0.7871 - val_loss: 0.7200 - val_acc: 0.7250\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.6329 - acc: 0.7880 - val_loss: 0.7138 - val_acc: 0.7270\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.6247 - acc: 0.7905 - val_loss: 0.7091 - val_acc: 0.7300\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.6171 - acc: 0.7891 - val_loss: 0.7030 - val_acc: 0.7280\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.6095 - acc: 0.7972 - val_loss: 0.6998 - val_acc: 0.7290\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.6023 - acc: 0.7949 - val_loss: 0.6945 - val_acc: 0.7310\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.5956 - acc: 0.7995 - val_loss: 0.6916 - val_acc: 0.7330\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.5887 - acc: 0.8000 - val_loss: 0.6857 - val_acc: 0.7370\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 0.5823 - acc: 0.8039 - val_loss: 0.6835 - val_acc: 0.7340\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.5762 - acc: 0.8055 - val_loss: 0.6803 - val_acc: 0.7360\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.5699 - acc: 0.8047 - val_loss: 0.6769 - val_acc: 0.7360\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.5643 - acc: 0.8096 - val_loss: 0.6753 - val_acc: 0.7410\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.5582 - acc: 0.8111 - val_loss: 0.6749 - val_acc: 0.7410\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.5537 - acc: 0.8128 - val_loss: 0.6698 - val_acc: 0.7380\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.5478 - acc: 0.8127 - val_loss: 0.6686 - val_acc: 0.7350\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.5427 - acc: 0.8145 - val_loss: 0.6654 - val_acc: 0.7420\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.5374 - acc: 0.8185 - val_loss: 0.6644 - val_acc: 0.7400\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.5324 - acc: 0.8200 - val_loss: 0.6639 - val_acc: 0.7360\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.5272 - acc: 0.8240 - val_loss: 0.6642 - val_acc: 0.7400\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.5232 - acc: 0.8217 - val_loss: 0.6583 - val_acc: 0.7350\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.5180 - acc: 0.8241 - val_loss: 0.6561 - val_acc: 0.7460\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.5136 - acc: 0.8249 - val_loss: 0.6548 - val_acc: 0.7460\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.5092 - acc: 0.8280 - val_loss: 0.6520 - val_acc: 0.7450\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.5049 - acc: 0.8303 - val_loss: 0.6522 - val_acc: 0.7370\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.5004 - acc: 0.8331 - val_loss: 0.6500 - val_acc: 0.7440\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.4962 - acc: 0.8331 - val_loss: 0.6495 - val_acc: 0.7390\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.4922 - acc: 0.8349 - val_loss: 0.6484 - val_acc: 0.7440\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 0.4875 - acc: 0.8383 - val_loss: 0.6496 - val_acc: 0.7400\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.4842 - acc: 0.8384 - val_loss: 0.6468 - val_acc: 0.7410\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.4804 - acc: 0.8416 - val_loss: 0.6448 - val_acc: 0.7450\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.4763 - acc: 0.8416 - val_loss: 0.6433 - val_acc: 0.7420\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.4720 - acc: 0.8437 - val_loss: 0.6454 - val_acc: 0.7470\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.4690 - acc: 0.8457 - val_loss: 0.6449 - val_acc: 0.7370\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.4651 - acc: 0.8456 - val_loss: 0.6405 - val_acc: 0.7440\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.4612 - acc: 0.8477 - val_loss: 0.6395 - val_acc: 0.7470\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.4578 - acc: 0.8496 - val_loss: 0.6386 - val_acc: 0.7470\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.4544 - acc: 0.8481 - val_loss: 0.6391 - val_acc: 0.7430\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.4508 - acc: 0.8515 - val_loss: 0.6386 - val_acc: 0.7460\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.4474 - acc: 0.8527 - val_loss: 0.6391 - val_acc: 0.7440\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.4436 - acc: 0.8532 - val_loss: 0.6390 - val_acc: 0.7420\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.4404 - acc: 0.8564 - val_loss: 0.6374 - val_acc: 0.7450\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.4371 - acc: 0.8565 - val_loss: 0.6371 - val_acc: 0.7480\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.4341 - acc: 0.8577 - val_loss: 0.6374 - val_acc: 0.7440\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.4306 - acc: 0.8587 - val_loss: 0.6384 - val_acc: 0.7480\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.4277 - acc: 0.8607 - val_loss: 0.6393 - val_acc: 0.7460\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.4242 - acc: 0.8616 - val_loss: 0.6370 - val_acc: 0.7460\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.4214 - acc: 0.8624 - val_loss: 0.6352 - val_acc: 0.7490\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.4181 - acc: 0.8639 - val_loss: 0.6337 - val_acc: 0.7480\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.4156 - acc: 0.8651 - val_loss: 0.6359 - val_acc: 0.7470\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.4118 - acc: 0.8665 - val_loss: 0.6362 - val_acc: 0.7500\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.4089 - acc: 0.8684 - val_loss: 0.6341 - val_acc: 0.7490\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.4060 - acc: 0.8695 - val_loss: 0.6378 - val_acc: 0.7500\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.4031 - acc: 0.8705 - val_loss: 0.6348 - val_acc: 0.7460\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.4006 - acc: 0.8715 - val_loss: 0.6359 - val_acc: 0.7440\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.3974 - acc: 0.8719 - val_loss: 0.6340 - val_acc: 0.7540\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.3948 - acc: 0.8737 - val_loss: 0.6361 - val_acc: 0.7520\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.3922 - acc: 0.8757 - val_loss: 0.6354 - val_acc: 0.7520\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.3893 - acc: 0.8761 - val_loss: 0.6342 - val_acc: 0.7530\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.3863 - acc: 0.8763 - val_loss: 0.6344 - val_acc: 0.7520\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.3835 - acc: 0.8772 - val_loss: 0.6411 - val_acc: 0.7510\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.3812 - acc: 0.8797 - val_loss: 0.6357 - val_acc: 0.7510\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.3786 - acc: 0.8799 - val_loss: 0.6347 - val_acc: 0.7530\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.3759 - acc: 0.8803 - val_loss: 0.6360 - val_acc: 0.7510\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.3732 - acc: 0.8807 - val_loss: 0.6364 - val_acc: 0.7470\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.3709 - acc: 0.8808 - val_loss: 0.6366 - val_acc: 0.7510\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.3678 - acc: 0.8835 - val_loss: 0.6360 - val_acc: 0.7500\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.3654 - acc: 0.8837 - val_loss: 0.6366 - val_acc: 0.7470\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.3631 - acc: 0.8840 - val_loss: 0.6365 - val_acc: 0.7470\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.3605 - acc: 0.8864 - val_loss: 0.6370 - val_acc: 0.7500\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.3585 - acc: 0.8853 - val_loss: 0.6394 - val_acc: 0.7500\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.3553 - acc: 0.8885 - val_loss: 0.6390 - val_acc: 0.7420\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.3530 - acc: 0.8903 - val_loss: 0.6395 - val_acc: 0.7470\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.3509 - acc: 0.8899 - val_loss: 0.6384 - val_acc: 0.7500\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.3481 - acc: 0.8905 - val_loss: 0.6380 - val_acc: 0.7460\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.3460 - acc: 0.8927 - val_loss: 0.6379 - val_acc: 0.7530\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.3435 - acc: 0.8913 - val_loss: 0.6424 - val_acc: 0.7550\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.3414 - acc: 0.8944 - val_loss: 0.6394 - val_acc: 0.7460\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.3390 - acc: 0.8921 - val_loss: 0.6424 - val_acc: 0.7460\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.3372 - acc: 0.8935 - val_loss: 0.6408 - val_acc: 0.7460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.3346 - acc: 0.8963 - val_loss: 0.6414 - val_acc: 0.7440\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the result similarly to what we have done in the previous lab. This time though, let's iclude the training and the validation loss in the same plot. We'll do the same thing for the training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a17c64c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'g.', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a17c7a4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = history_dict['acc'] \n",
    "val_acc_values = history_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'r.', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a status quo around the 50th epoch. This means that we're actually **overfitting** to the train data when we do as many epochs as we were doing. Luckily, you learned how to tackle overfitting in the previous lecture! For starters, it does seem clear that we are training too long. So let's stop training at the 50th epoch first (so-called \"early stopping\") before we move to more advanced regularization techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/50\n",
      "7500/7500 [==============================] - 1s 184us/step - loss: 1.9421 - acc: 0.1569 - val_loss: 1.9422 - val_acc: 0.1450\n",
      "Epoch 2/50\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.9232 - acc: 0.1736 - val_loss: 1.9281 - val_acc: 0.1560\n",
      "Epoch 3/50\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.9083 - acc: 0.1984 - val_loss: 1.9150 - val_acc: 0.1910\n",
      "Epoch 4/50\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.8939 - acc: 0.2221 - val_loss: 1.9017 - val_acc: 0.2110\n",
      "Epoch 5/50\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.8785 - acc: 0.2424 - val_loss: 1.8872 - val_acc: 0.2340\n",
      "Epoch 6/50\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.8615 - acc: 0.2589 - val_loss: 1.8710 - val_acc: 0.2500\n",
      "Epoch 7/50\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.8424 - acc: 0.2713 - val_loss: 1.8527 - val_acc: 0.2660\n",
      "Epoch 8/50\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.8207 - acc: 0.2949 - val_loss: 1.8314 - val_acc: 0.2850\n",
      "Epoch 9/50\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.7960 - acc: 0.3171 - val_loss: 1.8067 - val_acc: 0.3050\n",
      "Epoch 10/50\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.7678 - acc: 0.3448 - val_loss: 1.7788 - val_acc: 0.3280\n",
      "Epoch 11/50\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.7363 - acc: 0.3740 - val_loss: 1.7471 - val_acc: 0.3490\n",
      "Epoch 12/50\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.7011 - acc: 0.4005 - val_loss: 1.7111 - val_acc: 0.3920\n",
      "Epoch 13/50\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.6626 - acc: 0.4188 - val_loss: 1.6725 - val_acc: 0.4080\n",
      "Epoch 14/50\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.6210 - acc: 0.4477 - val_loss: 1.6321 - val_acc: 0.4270\n",
      "Epoch 15/50\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.5767 - acc: 0.4711 - val_loss: 1.5893 - val_acc: 0.4510\n",
      "Epoch 16/50\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.5306 - acc: 0.4888 - val_loss: 1.5452 - val_acc: 0.4670\n",
      "Epoch 17/50\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.4838 - acc: 0.5072 - val_loss: 1.5002 - val_acc: 0.4730\n",
      "Epoch 18/50\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.4369 - acc: 0.5304 - val_loss: 1.4555 - val_acc: 0.4930\n",
      "Epoch 19/50\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.3903 - acc: 0.5455 - val_loss: 1.4112 - val_acc: 0.5160\n",
      "Epoch 20/50\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.3446 - acc: 0.5643 - val_loss: 1.3671 - val_acc: 0.5270\n",
      "Epoch 21/50\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.3002 - acc: 0.5833 - val_loss: 1.3250 - val_acc: 0.5520\n",
      "Epoch 22/50\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.2567 - acc: 0.6023 - val_loss: 1.2846 - val_acc: 0.5690\n",
      "Epoch 23/50\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.2151 - acc: 0.6180 - val_loss: 1.2449 - val_acc: 0.5860\n",
      "Epoch 24/50\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.1751 - acc: 0.6320 - val_loss: 1.2093 - val_acc: 0.5840\n",
      "Epoch 25/50\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.1373 - acc: 0.6465 - val_loss: 1.1722 - val_acc: 0.6100\n",
      "Epoch 26/50\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.1012 - acc: 0.6627 - val_loss: 1.1411 - val_acc: 0.6060\n",
      "Epoch 27/50\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.0671 - acc: 0.6733 - val_loss: 1.1073 - val_acc: 0.6350\n",
      "Epoch 28/50\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0347 - acc: 0.6815 - val_loss: 1.0778 - val_acc: 0.6430\n",
      "Epoch 29/50\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.0046 - acc: 0.6879 - val_loss: 1.0497 - val_acc: 0.6410\n",
      "Epoch 30/50\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9759 - acc: 0.6965 - val_loss: 1.0235 - val_acc: 0.6610\n",
      "Epoch 31/50\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9493 - acc: 0.7059 - val_loss: 1.0000 - val_acc: 0.6580\n",
      "Epoch 32/50\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9239 - acc: 0.7104 - val_loss: 0.9791 - val_acc: 0.6610\n",
      "Epoch 33/50\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9009 - acc: 0.7169 - val_loss: 0.9581 - val_acc: 0.6620\n",
      "Epoch 34/50\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8781 - acc: 0.7215 - val_loss: 0.9367 - val_acc: 0.6700\n",
      "Epoch 35/50\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8573 - acc: 0.7271 - val_loss: 0.9168 - val_acc: 0.6790\n",
      "Epoch 36/50\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 0.8388 - acc: 0.733 - 0s 58us/step - loss: 0.8378 - acc: 0.7328 - val_loss: 0.9012 - val_acc: 0.6820\n",
      "Epoch 37/50\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8192 - acc: 0.7371 - val_loss: 0.8860 - val_acc: 0.6770\n",
      "Epoch 38/50\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8022 - acc: 0.7396 - val_loss: 0.8702 - val_acc: 0.6970\n",
      "Epoch 39/50\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.7859 - acc: 0.7457 - val_loss: 0.8568 - val_acc: 0.6960\n",
      "Epoch 40/50\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.7706 - acc: 0.7503 - val_loss: 0.8439 - val_acc: 0.6990\n",
      "Epoch 41/50\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.7562 - acc: 0.7519 - val_loss: 0.8328 - val_acc: 0.7020\n",
      "Epoch 42/50\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.7424 - acc: 0.7552 - val_loss: 0.8197 - val_acc: 0.7080\n",
      "Epoch 43/50\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.7299 - acc: 0.7596 - val_loss: 0.8130 - val_acc: 0.7030\n",
      "Epoch 44/50\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.7172 - acc: 0.7628 - val_loss: 0.8066 - val_acc: 0.7040\n",
      "Epoch 45/50\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.7062 - acc: 0.7635 - val_loss: 0.7919 - val_acc: 0.7130\n",
      "Epoch 46/50\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.6947 - acc: 0.7687 - val_loss: 0.7835 - val_acc: 0.7140\n",
      "Epoch 47/50\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.6842 - acc: 0.7697 - val_loss: 0.7757 - val_acc: 0.7150\n",
      "Epoch 48/50\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.6750 - acc: 0.7735 - val_loss: 0.7695 - val_acc: 0.7190\n",
      "Epoch 49/50\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.6650 - acc: 0.7771 - val_loss: 0.7604 - val_acc: 0.7300\n",
      "Epoch 50/50\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.6562 - acc: 0.7777 - val_loss: 0.7587 - val_acc: 0.7230\n",
      "1500/1500 [==============================] - 0s 91us/step\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "final_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=50,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use the test set to make label predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 61us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 78us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6503562787055969, 0.7776000000317892]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7675205413500468, 0.7166666661898295]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've significantly reduced the variance, but now our test results are actually also worse comparing with the results we obtained before. Let's see what else we can do!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 2s 203us/step - loss: 4.5245 - acc: 0.1604 - val_loss: 4.4713 - val_acc: 0.1880\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 4.4357 - acc: 0.2099 - val_loss: 4.3908 - val_acc: 0.2220\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 4.3557 - acc: 0.2492 - val_loss: 4.3140 - val_acc: 0.2420\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 4.2773 - acc: 0.2712 - val_loss: 4.2373 - val_acc: 0.2620\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 4.1986 - acc: 0.2853 - val_loss: 4.1594 - val_acc: 0.2750\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 4.1191 - acc: 0.2976 - val_loss: 4.0805 - val_acc: 0.2980\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 4.0389 - acc: 0.3135 - val_loss: 3.9995 - val_acc: 0.3120\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 3.9578 - acc: 0.3284 - val_loss: 3.9185 - val_acc: 0.3380\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 3.8760 - acc: 0.3412 - val_loss: 3.8369 - val_acc: 0.3630\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 3.7941 - acc: 0.3625 - val_loss: 3.7560 - val_acc: 0.3620\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 3.7129 - acc: 0.3717 - val_loss: 3.6747 - val_acc: 0.3970\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 3.6322 - acc: 0.3959 - val_loss: 3.5950 - val_acc: 0.4170\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 3.5528 - acc: 0.4121 - val_loss: 3.5166 - val_acc: 0.4530\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 3.4749 - acc: 0.4429 - val_loss: 3.4395 - val_acc: 0.4680\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 3.3981 - acc: 0.4653 - val_loss: 3.3644 - val_acc: 0.4770\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 3.3228 - acc: 0.4823 - val_loss: 3.2909 - val_acc: 0.4940\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 3.2492 - acc: 0.5039 - val_loss: 3.2186 - val_acc: 0.5220\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 3.1777 - acc: 0.5299 - val_loss: 3.1482 - val_acc: 0.5300\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 3.1075 - acc: 0.5451 - val_loss: 3.0813 - val_acc: 0.5390\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 3.0402 - acc: 0.5652 - val_loss: 3.0145 - val_acc: 0.5530\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 2.9741 - acc: 0.579 - 0s 51us/step - loss: 2.9745 - acc: 0.5781 - val_loss: 2.9514 - val_acc: 0.5710\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 2.9111 - acc: 0.5980 - val_loss: 2.8901 - val_acc: 0.5780\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 2.8501 - acc: 0.6143 - val_loss: 2.8319 - val_acc: 0.5920\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 2.7913 - acc: 0.6255 - val_loss: 2.7739 - val_acc: 0.6060\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 2.7344 - acc: 0.6365 - val_loss: 2.7203 - val_acc: 0.6280\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 2.6800 - acc: 0.6544 - val_loss: 2.6684 - val_acc: 0.6390\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 2.6273 - acc: 0.6619 - val_loss: 2.6190 - val_acc: 0.6580\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 2.5765 - acc: 0.6739 - val_loss: 2.5708 - val_acc: 0.6660\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 2.5276 - acc: 0.6853 - val_loss: 2.5232 - val_acc: 0.6670\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 2.4802 - acc: 0.6915 - val_loss: 2.4790 - val_acc: 0.6810\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 2.4344 - acc: 0.6977 - val_loss: 2.4350 - val_acc: 0.6810\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 2.3904 - acc: 0.7049 - val_loss: 2.3942 - val_acc: 0.6850\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 2.3478 - acc: 0.7124 - val_loss: 2.3540 - val_acc: 0.6920\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 2.3063 - acc: 0.7180 - val_loss: 2.3157 - val_acc: 0.6950\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 2.2670 - acc: 0.7220 - val_loss: 2.2787 - val_acc: 0.6980\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 2.2282 - acc: 0.7283 - val_loss: 2.2424 - val_acc: 0.6960\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 2.1912 - acc: 0.7311 - val_loss: 2.2074 - val_acc: 0.7010\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 2.1550 - acc: 0.7380 - val_loss: 2.1743 - val_acc: 0.7070\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 2.1204 - acc: 0.7425 - val_loss: 2.1411 - val_acc: 0.7050\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 2.0861 - acc: 0.7475 - val_loss: 2.1090 - val_acc: 0.7090\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 2.0533 - acc: 0.7488 - val_loss: 2.0780 - val_acc: 0.7120\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 2.0214 - acc: 0.7541 - val_loss: 2.0482 - val_acc: 0.7150\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.9908 - acc: 0.7544 - val_loss: 2.0192 - val_acc: 0.7230\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.9608 - acc: 0.7572 - val_loss: 1.9921 - val_acc: 0.7230\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.9322 - acc: 0.7613 - val_loss: 1.9664 - val_acc: 0.7190\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.9044 - acc: 0.7617 - val_loss: 1.9397 - val_acc: 0.7260\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.8774 - acc: 0.7663 - val_loss: 1.9160 - val_acc: 0.7260\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.8515 - acc: 0.7660 - val_loss: 1.8916 - val_acc: 0.7310\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.8260 - acc: 0.7665 - val_loss: 1.8721 - val_acc: 0.7220\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.8018 - acc: 0.7731 - val_loss: 1.8457 - val_acc: 0.7310\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.7780 - acc: 0.7735 - val_loss: 1.8243 - val_acc: 0.7350\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.7551 - acc: 0.7735 - val_loss: 1.8032 - val_acc: 0.7290\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.7326 - acc: 0.7751 - val_loss: 1.7820 - val_acc: 0.7310\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.7108 - acc: 0.7763 - val_loss: 1.7612 - val_acc: 0.7370\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.6900 - acc: 0.7820 - val_loss: 1.7431 - val_acc: 0.7320\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.6694 - acc: 0.7800 - val_loss: 1.7244 - val_acc: 0.7320\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.6495 - acc: 0.7804 - val_loss: 1.7071 - val_acc: 0.7330\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.6303 - acc: 0.7863 - val_loss: 1.6874 - val_acc: 0.7310\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.6116 - acc: 0.7861 - val_loss: 1.6700 - val_acc: 0.7320\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.5936 - acc: 0.7828 - val_loss: 1.6536 - val_acc: 0.7370\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.5754 - acc: 0.7873 - val_loss: 1.6361 - val_acc: 0.7380\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.5584 - acc: 0.7876 - val_loss: 1.6211 - val_acc: 0.7390\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.5415 - acc: 0.7899 - val_loss: 1.6064 - val_acc: 0.7380\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.5247 - acc: 0.7899 - val_loss: 1.5900 - val_acc: 0.7330\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.5093 - acc: 0.7928 - val_loss: 1.5761 - val_acc: 0.7340\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.4934 - acc: 0.7920 - val_loss: 1.5620 - val_acc: 0.7350\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.4780 - acc: 0.7951 - val_loss: 1.5488 - val_acc: 0.7400\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.4635 - acc: 0.7945 - val_loss: 1.5343 - val_acc: 0.7410\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.4489 - acc: 0.7973 - val_loss: 1.5234 - val_acc: 0.7410\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.4348 - acc: 0.7939 - val_loss: 1.5081 - val_acc: 0.7360\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.4211 - acc: 0.8035 - val_loss: 1.4958 - val_acc: 0.7390\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.4077 - acc: 0.8007 - val_loss: 1.4837 - val_acc: 0.7410\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.3945 - acc: 0.8011 - val_loss: 1.4710 - val_acc: 0.7440\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.3819 - acc: 0.8023 - val_loss: 1.4605 - val_acc: 0.7410\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.3696 - acc: 0.8021 - val_loss: 1.4497 - val_acc: 0.7400\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.3574 - acc: 0.8065 - val_loss: 1.4408 - val_acc: 0.7380\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.3455 - acc: 0.8045 - val_loss: 1.4273 - val_acc: 0.7430\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.3337 - acc: 0.8071 - val_loss: 1.4178 - val_acc: 0.7400\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.3225 - acc: 0.8017 - val_loss: 1.4058 - val_acc: 0.7400\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.3116 - acc: 0.8085 - val_loss: 1.3973 - val_acc: 0.7400\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.3009 - acc: 0.8063 - val_loss: 1.3862 - val_acc: 0.7390\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.2901 - acc: 0.8081 - val_loss: 1.3772 - val_acc: 0.7400\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.2797 - acc: 0.8096 - val_loss: 1.3676 - val_acc: 0.7390\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.2697 - acc: 0.8084 - val_loss: 1.3583 - val_acc: 0.7420\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.2599 - acc: 0.8147 - val_loss: 1.3506 - val_acc: 0.7430\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.2503 - acc: 0.8091 - val_loss: 1.3410 - val_acc: 0.7430\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.2408 - acc: 0.8136 - val_loss: 1.3334 - val_acc: 0.7410\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.2314 - acc: 0.8107 - val_loss: 1.3270 - val_acc: 0.7430\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.2225 - acc: 0.8123 - val_loss: 1.3167 - val_acc: 0.7420\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.2137 - acc: 0.8140 - val_loss: 1.3091 - val_acc: 0.7380\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.2052 - acc: 0.8148 - val_loss: 1.3015 - val_acc: 0.7430\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.1969 - acc: 0.8173 - val_loss: 1.2935 - val_acc: 0.7400\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.1886 - acc: 0.8156 - val_loss: 1.2887 - val_acc: 0.7340\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.1804 - acc: 0.8161 - val_loss: 1.2811 - val_acc: 0.7390\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.1730 - acc: 0.8217 - val_loss: 1.2710 - val_acc: 0.7430\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.1649 - acc: 0.8176 - val_loss: 1.2642 - val_acc: 0.7450\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.1573 - acc: 0.8211 - val_loss: 1.2580 - val_acc: 0.7380\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.1502 - acc: 0.8203 - val_loss: 1.2550 - val_acc: 0.7400\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.1430 - acc: 0.8179 - val_loss: 1.2452 - val_acc: 0.7430\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.1357 - acc: 0.8240 - val_loss: 1.2398 - val_acc: 0.7410\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.1289 - acc: 0.8228 - val_loss: 1.2331 - val_acc: 0.7430\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.1224 - acc: 0.8229 - val_loss: 1.2259 - val_acc: 0.7420\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.1153 - acc: 0.8237 - val_loss: 1.2212 - val_acc: 0.7380\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.1089 - acc: 0.8269 - val_loss: 1.2181 - val_acc: 0.7390\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.1029 - acc: 0.8261 - val_loss: 1.2108 - val_acc: 0.7380\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.0967 - acc: 0.8247 - val_loss: 1.2042 - val_acc: 0.7410\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0904 - acc: 0.8283 - val_loss: 1.2000 - val_acc: 0.7430\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.0843 - acc: 0.8265 - val_loss: 1.1954 - val_acc: 0.7370\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.0789 - acc: 0.8293 - val_loss: 1.1903 - val_acc: 0.7340\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0731 - acc: 0.8300 - val_loss: 1.1844 - val_acc: 0.7430\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.0676 - acc: 0.8256 - val_loss: 1.1799 - val_acc: 0.7400\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.0619 - acc: 0.8300 - val_loss: 1.1753 - val_acc: 0.7400\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0566 - acc: 0.8285 - val_loss: 1.1706 - val_acc: 0.7380\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.0512 - acc: 0.8295 - val_loss: 1.1661 - val_acc: 0.7360\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.0461 - acc: 0.8329 - val_loss: 1.1616 - val_acc: 0.7420\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.0412 - acc: 0.8320 - val_loss: 1.1580 - val_acc: 0.7380\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.0365 - acc: 0.8331 - val_loss: 1.1531 - val_acc: 0.7370\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.0316 - acc: 0.8312 - val_loss: 1.1489 - val_acc: 0.7420\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0267 - acc: 0.8315 - val_loss: 1.1467 - val_acc: 0.7380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0219 - acc: 0.8356 - val_loss: 1.1398 - val_acc: 0.7390\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l2(0.02), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l2(0.02), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "final_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOURCES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://catalog.data.gov/dataset/consumer-complaint-database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
